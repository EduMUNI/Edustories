# -*- coding: utf-8 -*-
"""Based on a Jupyter notebook "Celý skript final Case_study_GPT4 8.1.ipynb"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1If-xRkUULzVGd06BhtvELs19nz2FF99E

Cílem tohoto skriptu je dostat z předem definované nestrukturované matice. Použitelnou matici pro práci s kazuistikami. Ze složky načítání kazuistik je zde: https://colab.research.google.com/drive/1M5tukroXLdPv7M7jKhbTnDXVdmTtZUvb?usp=sharing

## Načtení OpenAI
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install openai # %

import openai

import os
os.environ["OPENAI_API_KEY"] = "TODO:replace with your key"

from openai import OpenAI
client = OpenAI()

"""## Načtení všech kazuistik"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Připojení Google Drive

# Cesta k vašemu souboru na Google Drive
# Nahraďte 'cesta/ke/souboru.txt' skutečnou cestou k vašemu souboru
file_path = 'kazuistiky-all-opr.txt'

# Načtení souboru do DataFrame
df = pd.read_csv(file_path, sep='\t', encoding='utf-8')

# Zobrazení prvních několika řádků DataFrame
print(df.head())

df.insert(0, 'content_original', df['content'])

#print(df)

"""# Extrakce částí kazuistik

"""

#Bacha!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

import pandas as pd
import re

# Odstranění zalomení řádků
df['content'] = df['content'].apply(lambda x: x.replace('\n', ' '))


# Odstranění zalomení řádků a více než dvou mezer, zároveň zachování alespoň jedné mezery mezi slovy
#df['content'] = df['content'].apply(lambda x: re.sub(r'\s{2,}', ' ', x.replace('\n', ' ')))

"""## Extrakce entit - učitele"""

import pandas as pd
import re

# Předpokládám, že 'df' je váš DataFrame
# Přidávám nové sloupce pro každou entitu
df['Aprobace'] = ''
df['Roky praxe'] = ''
df['Kurz'] = ''

# Funkce pro extrakci entity pomocí regulárního výrazu
def extract_entity(text, start_pattern, end_pattern):
    pattern = re.compile(start_pattern + '(.*?)' + end_pattern, re.DOTALL)
    match = pattern.search(text)
    return match.group(1).strip() if match else ''

# Iterace přes DataFrame a extrakce entit
for index, row in df.iterrows():
    content = row['content']

    # Extrahování entity "Aprobace"
    aprobace = extract_entity(content, '\\(aprobace\\)\\?', 'Počet let')
    if not aprobace:
        aprobace = extract_entity(content, '\\(aprobace\\)\\?', 'Praxe:')
    if not aprobace:
        aprobace = extract_entity(content, 'Aprobace:', 'Praxe:')
    if not aprobace:
        aprobace = extract_entity(content, 'Aprobace:', 'Počet let')
    if not aprobace:
        aprobace = extract_entity(content, 'aprobace:', 'Počet let')
    if not aprobace:
        aprobace = extract_entity(content, '\\(aprobace\\):', 'Počet let')
    if not aprobace:
        aprobace = extract_entity(content, '\\(aprobace\\)', 'Počet let')

    df.at[index, 'Aprobace'] = aprobace

    # Extrahování entity "Roky praxe" Praxe: 5let Absolvoval/a
    roky_praxe = extract_entity(content, 'let praxe:', 'Absolvova.*')
    if not roky_praxe:
        roky_praxe = extract_entity(content, 'praxe:', 'praxe')
    if not roky_praxe:
        roky_praxe = extract_entity(content, 'praxe:', 'Relevantní')
    if not roky_praxe:
        roky_praxe = extract_entity(content, ' praxe:', 'Absolvování')
    if not roky_praxe:
        roky_praxe = extract_entity(content, 'praxe:', 'Neabsolvovala')
    if not roky_praxe:
        roky_praxe = extract_entity(content, 'praxe:', 'Absolvoval/a')

    df.at[index, 'Roky praxe'] = roky_praxe

    # Extrahování entity "Kurz"
    # ...
    # Původní kód pro extrakci entity "kurz"
    kurz = extract_entity(content, '\\(zatrhněte\\):', 'Případně')
    if not kurz:
        kurz = extract_entity(content, 'podobně:', 'Případně')
    if not kurz:
        kurz = extract_entity(content, '\\(zatrhněte', 'Případně')
    # Další vzory pro extrakci
    if not kurz:
        kurz = extract_entity(content, 'problematických situací:', 'Případně')
    if not kurz:
        kurz = extract_entity(content, 've třídě a podobně :', 'Případně')
    if not kurz:
        kurz = extract_entity(content, 'komunikace ve třídě:', 'Případně')
    if not kurz:
        kurz = extract_entity(content, 'třídě a podobně:', 'Kazuistika \\+\\+')
    if not kurz:
        kurz = extract_entity(content, 'a podobně:', 'Kazuistika \\+\\+')
    if not kurz:
        kurz = extract_entity(content, 'a podobně?', 'Kazuistika \\+\\+')
    if not kurz:
        kurz = extract_entity(content, 'a podobně:', 'Název:')
    if not kurz:
        kurz = extract_entity(content, 'a podobně \\(zatrhněte\\):', 'Kazuistika \\+\\+')
    if not kurz:
        kurz = extract_entity(content, 'a podobně:', 'Deskriptivní')
    if not kurz:
        kurz = extract_entity(content, 'problematických situací:', 'Název')

    df.at[index, 'Kurz'] = kurz



    df.at[index, 'Kurz'] = kurz

# Výpis výsledku
#print(df)

import seaborn as sns
import matplotlib.pyplot as plt

# Definování sloupců pro analýzu
entity_columns = ['Aprobace', 'Roky praxe', 'Kurz']

# Přeformátování DataFrame pro graf
melted_df_ent = df.melt(value_vars=entity_columns, var_name='Entity', value_name='Value')
melted_df_ent['Is Empty'] = melted_df_ent['Value'].str.strip().eq('')

# Počítání prázdných hodnot pro každou entitu
empty_counts = melted_df_ent.groupby('Entity')['Is Empty'].sum()

# Barvy pro jednotlivé sloupce
colors = ['blue', 'green', 'red']

# Vykreslení grafu s různými barvami
barplot = sns.barplot(x=empty_counts.index, y=empty_counts.values, palette=colors)
plt.xlabel('Entita')
plt.ylabel('Počet prázdných hodnot')
plt.title('Počet prázdných hodnot pro vybrané entity')
plt.xticks(rotation=45)

# Přidání četností k jednotlivým sloupcům
for idx, value in enumerate(empty_counts.values):
    barplot.text(idx, value, str(value), color='black', ha="center")

plt.show()

# Filtrování řádků, kde je 'Aprobace' prázdná
empty_aprobace_df = df[df['Roky praxe'].str.strip() == '']

# Výběr prvních 5 kazuistik
selected_kazuistiky = empty_aprobace_df.head(25)

# Vypsání názvů souborů a příslušného obsahu
for index, row in selected_kazuistiky.iterrows():
    print(f"File Name: {row['file_name']}")
    print(f"Content:\n{row['content']}\n")
    print('--------------------------------------\n')

"""## Rozdělení na dvě kazuistiky"""

# Rozdělení na dvě kazuistiky ++ a -- a zachování extrakce entit od učitele

# Inicializace nového DataFrame pro uložení výsledků
new_rows = []

# Iterace přes každý řádek v df
for index, row in df.iterrows():
    content_parts = row['content'].split("Do jaké míry", 1)  # Rozdělení textu
    file_name = row['file_name']
    aprobace = row['Aprobace']
    roky_praxe = row['Roky praxe']
    kurz = row['Kurz']
    content_parts_org = row['content_original'].split("Do jaké míry", 1)

    # Přidání první části kazuistiky
    new_rows.append({'file_name': file_name + "01", 'content': "Do jaké míry" + content_parts[0], 'content_original': "Do jaké míry" + content_parts_org[0], 'Aprobace': aprobace, 'Roky praxe': roky_praxe, 'Kurz': kurz})

    # Přidání druhé části kazuistiky, pokud existuje
    if len(content_parts) > 1:
        new_rows.append({'file_name': file_name + "02", 'content': "Do jaké míry" + content_parts[1], 'content_original': "Do jaké míry" + content_parts_org[1], 'Aprobace': aprobace, 'Roky praxe': roky_praxe, 'Kurz': kurz})

# Vytvoření nového DataFrame z rozdělených řádků
new_df = pd.DataFrame(new_rows)

# Výpis nového DataFrame
#print(new_df)

# print(new_df)

"""## Extrakce entit - kazusitiky ++/-- a další"""

import pandas as pd
import re

# Function to search for specific patterns
def find_pattern(text):
    # Pattern for "++" variants
    plus_pattern = r'ke\s+kazuistice\s+\+[\s\+]+'

    # Pattern for "--" variants
    minus_pattern = r'ke\s+kazuistice\s+-[\s\-]+'


    if re.search(plus_pattern, text):
        return '++'
    elif re.search(minus_pattern, text):
        return '--'
    else:
        return 'NA'

# Apply the function to the DataFrame
new_df['outcome'] = new_df['content'].apply(find_pattern)

#new_df

# Count the number of occurrences of each result type
result_counts = new_df['outcome'].value_counts()

# Display the counts
print(result_counts)

import pandas as pd
import re

patterns_to_remove = [', \.\.\.\):', ',\.\.\.\):', ', \.\.\):', ',\.\.\):', '\.\.\):', ', …\):', ',…\)', ',\.\.\):', ' ,\.\.\):', 'otevřená otázka\): ', '\)\:']

# Funkce pro odstranění všech vzorů
def remove_patterns(text, patterns):
    for pattern in patterns:
        text = re.sub(pattern, '', text)
    return text

# Aplikace funkce na sloupec 'content'
new_df['content'] = new_df['content'].apply(lambda x: remove_patterns(x, patterns_to_remove))

import pandas as pd
import re

# Předpokládám, že 'new_df' je váš DataFrame
# Přidávám nové sloupce pro každou entitu
new_df['Věk'] = ''
new_df['Poruchy'] = ''
new_df['Diagnóza'] = ''
new_df['Zájmy'] = ''

# Funkce pro extrakci entity pomocí regulárního výrazu
def extract_entity(text, start_pattern, end_pattern):
    pattern = re.compile(start_pattern + '(.*?)' + end_pattern, re.DOTALL)
    match = pattern.search(text)
    return match.group(1).strip() if match else ''

# Iterace přes DataFrame a extrakce entit
for index, row in new_df.iterrows():
    content = row['content']
    new_df.at[index, 'Věk'] = extract_entity(content, 'ročník:', 'Pohlaví')

    # První pokus o extrakci entity "Poruchy"
    poruchy = extract_entity(content, 'podvody', 'Potvrzená')
    # Pokud nebyla entita nalezena, použijte alternativní vzor
    if not poruchy:
        poruchy = extract_entity(content, 'Poruchy chování:', 'Potvrzená')
    if not poruchy:
        poruchy = extract_entity(content, 'Poruchy chovania:', 'Potvrdená')
    if not poruchy:
        poruchy = extract_entity(content, 'Poruchy chovania:', 'Diagnóza:')
    if not poruchy:
        poruchy = extract_entity(content, 'podvod.', 'Potvrzená')

    new_df.at[index, 'Poruchy'] = poruchy

    # První pokus o extrakci entity "Diagnóza"
    diagnoza = extract_entity(content, 'aj\.', 'Prospěch')
    # Pokud nebyla entita nalezena, použijte první alternativní vzor
    if not diagnoza:
        diagnoza = extract_entity(content, 'diagnóza žáka:', 'Prospěch')
    # Pokud stále nebyla entita nalezena, použijte druhý alternativní vzor
    if not diagnoza:
        diagnoza = extract_entity(content, 'diagnóza žáka/ů', 'Prospěch')

    new_df.at[index, 'Diagnóza'] = diagnoza

    # První pokus o extrakci entity "zajmy"
    zajmy = extract_entity(content, 'Zájmy žáka:', 'Použili')
    # Pokud nebyla entita nalezena, použijte alternativní vzor
    if not zajmy:
        zajmy = extract_entity(content, 'Zájmy žáka/ů', 'Použili')
    if not zajmy:
        zajmy = extract_entity(content, 'Zájmy žáka', 'Použili')
    if not zajmy:
        zajmy = extract_entity(content, 'Zájmy žáka:', 'Použití')
    if not zajmy:
        zajmy = extract_entity(content, 'Zájmy žáka/ů', 'Podrobný')
    if not zajmy:
        zajmy = extract_entity(content, 'Zájmy žáků', 'Použili')
    if not zajmy:
        zajmy = extract_entity(content, 'Zájmy žáka/ů', 'Postupy')

    new_df.at[index, 'Zájmy'] = zajmy

# Výpis výsledku
#print(new_df)

# Definování sloupců pro analýzu
entity_columns = ['Aprobace', 'Roky praxe', 'Kurz', 'Věk', 'Poruchy', 'Diagnóza', 'Zájmy']

# Přeformátování DataFrame pro graf
melted_df1 = new_df.melt(value_vars=entity_columns, var_name='Entity', value_name='Value')
melted_df1['Is Empty'] = melted_df1['Value'].str.strip().eq('')

# Počítání prázdných hodnot pro každou entitu
empty_counts = melted_df1.groupby('Entity')['Is Empty'].sum()

# Vykreslení grafu
ax = sns.barplot(x=empty_counts.index, y=empty_counts.values)
plt.xlabel('Entita')
plt.ylabel('Počet prázdných hodnot')
plt.title('Počet prázdných hodnot pro každou entitu')
plt.xticks(rotation=45)

# Přidání hodnot nad sloupce
for p in ax.patches:
    ax.annotate(f"{int(p.get_height())}",
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center',
                xytext=(0, 9),
                textcoords='offset points')

plt.show()

# # Filtrování řádků, kde je 'Aprobace' prázdná
# empty_aprobace_df = new_df[new_df['Zájmy'].str.strip() == '']

# # Výběr prvních 5 kazuistik
# #empty_aprobace_df[50:100]
# selected_kazuistiky = empty_aprobace_df.head(100)

# # Vypsání názvů souborů a příslušného obsahu
# for index, row in selected_kazuistiky.iterrows():
#     print(f"File Name: {row['file_name']}")
#     print(f"Content:\n{row['content']}\n")
#     print('--------------------------------------\n')

# # Název souboru kazuistiky, který hledáte
# nazev_souboru = "494811_-_Tomastikova_-_Kazuistika.txt"

# # Najděte řádek s hledaným názvem souboru v anonymizovaném DataFrame
# radek = new_df[new_df['file_name'] == nazev_souboru]

# # Zkontrolujte, zda existuje odpovídající řádek
# if not radek.empty:
#     # Vypíše anonymizovaný text kazuistiky z prvního nalezeného řádku
#     print(radek.iloc[0]['content'])
# else:
#     print("Kazuistika s daným názvem souboru nebyla nalezena.")

# ukazka = radek.iloc[0]['content']
# print(ukazka)

# odstranění části před "podrobný popis"

new_df2 = new_df.copy()

# Iterace přes každý řádek v new_df
for index, row in new_df2.iterrows():
    content = row['content']
    index_podrobny_popis = content.find("Podrobný popis")

    # Zkontrolovat, zda existuje fráze "Podrobný popis" a odstranit text před ní
    if index_podrobny_popis != -1:
        new_df2.at[index, 'content'] = content[index_podrobny_popis:]

# Výpis new_df2
#print(new_df2)

#Pozor kod pro odstranění "Do jaké míry" na konci kazuistiky
import pandas as pd

# Předpokládá se, že new_df2 je již načtený DataFrame
# Pokud je potřeba načíst DataFrame, přidejte zde příslušný kód

def odstranit_text_po_frazi(text, fraze):
    index = text.find(fraze)
    if index != -1:
        return text[:index + len(fraze)]
    else:
        return text

# Aplikace funkce na každý řádek ve sloupci 'content'
new_df2['content'] = new_df2['content'].apply(lambda x: odstranit_text_po_frazi(x, "Do jaké míry"))

# new_df2 nyní obsahuje upravený sloupec 'content'

import pandas as pd

# Předpokládá se, že new_df2 je již načtený DataFrame
# Pokud je potřeba načíst DataFrame, přidejte zde příslušný kód

# Filtrujeme řádky, kde délka obsahu ve sloupci 'content' je menší než 750 znaků
# problem: Kazuistika_1..txt02, Kazuistika_---converted.txt02
mene_nez_600_znaku = new_df2[new_df2['content'].str.len() < 800]

# Vypsat názvy souborů z těchto řádků
file_names = mene_nez_600_znaku['file_name']

print("Názvy souborů s méně než 750 znaky ve sloupci 'content':")
print(file_names)

import pandas as pd

# Předpokládá se, že new_df2 je již načtený DataFrame
# Pokud je potřeba načíst DataFrame, přidejte zde příslušný kód

# Filtrujeme řádky, kde délka obsahu ve sloupci 'content' je menší než 800 znaků
mene_nez_800_znaku = new_df2[new_df2['content'].str.len() < 800]

# Seřazení těchto řádků podle délky obsahu ve sloupci 'content' sestupně
serazene_podle_delky = mene_nez_800_znaku.sort_values(by='content', key=lambda x: x.str.len(), ascending=False)

# Výběr prvních 5 (nebo 10) řádků
top_5 = serazene_podle_delky.head(5)  # Pro 5 nejdelších
# top_10 = serazene_podle_delky.head(10)  # Pro 10 nejdelších

# Vypsat názvy souborů z těchto řádků
print("Názvy souborů s největším počtem znaků (do 800) ve sloupci 'content':")
print(top_5['file_name'])
# print(top_10['file_name'])  # Pro 10 nejdelších

# Předpokládá se, že new_df2 je již načtený DataFrame
# Pokud je potřeba načíst DataFrame, přidejte zde příslušný kód

# Vybrání konkrétního řádku podle názvu souboru
specific_case = new_df2.loc[new_df2['file_name'] == 'Ston_494464_Kazuistiky.txt02']

# Vypsání obsahu sloupce 'content' pro tuto kazuistiku
specific_case['content'].iloc[0]

# bacha!!!!!!!!!! toto odastraní kazusitiky kratší než 800 znaků (vím, že na tomto datsetu jsou to blbosti)
import pandas as pd

# Předpokládá se, že new_df2 je již načtený DataFrame
# Pokud je potřeba načíst DataFrame, přidejte zde příslušný kód

# Vyfiltrování řádků, kde je délka obsahu ve sloupci 'content' větší nebo rovna 800 znaků
new_df2 = new_df2[new_df2['content'].str.len() >= 800]

# new_df2 nyní obsahuje pouze řádky s obsahem délky 800 znaků nebo více

# # Předpokládáme, že máte již definovaný DataFrame new_df2 s existujícím sloupcem 'content'

# # Seznam názvů souborů, které chceme vyfiltrovat
# file_names = [
#     "Kazuistika_podzim_2021_praxe_3_asistent.txt02",
#     "Kazuistika_1__2_Adam_Staral__495489_.txt02",
# ]

# # Filtrace DataFrame podle názvů souborů
# filtered_df = new_df2[new_df2['file_name'].isin(file_names)]

# # Přejmenování sloupce 'content' na 'anonymized_content'
# #filtered_df = filtered_df.rename(columns={'content': 'anonymized_content'})

# # Výpis výsledků
# print(filtered_df)

# Krok 1: Získání seznamu sloupců a jejich aktuálního pořadí
columns = list(new_df2.columns)

# Krok 2: Zjistění indexu sloupce 'content_original'
content_original_index = columns.index('content_original')

# Krok 3: Přesunutí sloupce 'outcome' ihned za 'content_original'
# Nejprve odstraníme sloupec 'outcome' ze seznamu
columns.remove('outcome')
# Následně jej vložíme na požadovanou pozici
columns.insert(content_original_index + 1, 'outcome')

# Krok 4: Reorganizace DataFrame podle nového pořadí sloupců
new_df2 = new_df2[columns]

# # Export DataFrame do souboru CSV s tabulátorem jako separátorem a kódováním UTF-8
# new_df2.to_csv('new_df2.csv', sep='\t', index=False, encoding='utf-8')

# # Ke stažení souboru do vašeho lokálního systému
# from google.colab import files
# files.download('new_df2.csv')

"""### Nastavení velikosti vstupu na anonynizaci"""

# vytvoří df_subset1, df_subset2,...


# Předpokládám, že máte DataFrame nazvaný `new_df2` s 1543 řádky
# Pokud je název vašeho DataFrame jiný, nahraďte ho

# Počet řádků v každém subsetu
# rows_per_subset = 300
#
# # Vytvoření seznamu DataFrames
# subsets = [new_df2.iloc[i:i + rows_per_subset] for i in range(0, len(new_df2), rows_per_subset)]

# Pokud chcete přiřadit každý subset k proměnné (např. df_subset1, df_subset2, atd.), můžete to udělat takto:
# Tohle vytvoří proměnné df_subset1, df_subset2, atd. pro každý subset
# for i, subset in enumerate(subsets, start=1):
#     globals()[f'df_subset{i}'] = subset

# override of the BS above
# new_df2 = new_df


# print(df_subset2.head(2))

df_subset2 = new_df2

# Kontrola výsledku
#print(df_subset2)

"""# Anonymizace

* https://ufal.mff.cuni.cz/nametag/2/formats#output_formats
* Zkratky:
* https://ufal.mff.cuni.cz/~strakova/cnec2.0/ne-type-hierarchy.pdf

Types of NE - ty jedno písmené nějak nefungují....
* a - Numbers in addresses
* g - Geographical names
* i - Institutions
* m - Media names
* n - Number expressions
* o - Artifact names
* p - Personal names
* t - Time expressions
--
* ah - street numbers
* at - phone/fax numbers
* az - zip codes
* gc - states
* gh - hydronyms gl - nature areas / objects
* gq - urban parts gr - territorial names
* gs - streets, squares gt - continents
* gu - cities/towns g_ - underspecified
* ia - conferences/contests
* ic - cult./educ./scient. inst.
* if - companies, concerns... io - government/political inst.
* i_ - underspecified
* me - email address
* mi - internet links
* mn - periodical
* ms - radio and TV stations
* na - age
* nb - vol./page/chap./sec./fig. numbers nc - cardinal numbers
* ni - itemizer
* no - ordinal numbers
* ns - sport score n_ - underspecified
* oa - cultural artifacts (books, movies) oe - measure units
* om - currency units op - products
* or - directives, norms o_ - underspecified
* pc - inhabitant names
* pm - second names
* pd - (academic) titles
* pf - first names pm - second names
* pp - relig./myth persons
* ps - surnames
* p_ - underspecified td - days
* tf - feasts th - hours
* tm - months
* ty - years
* NE containers
* P - complex person names
* T - complex time expressions
* A - complex address expressions
"""

# df_subset2 = filtered_df

num_samples = None

if num_samples and num_samples is not None:
    df_subset2 = df_subset2.iloc[-num_samples:]

import requests
import json
import re
import time
import pandas as pd

# Vytvoření regulárního výrazu pro výjimky
exceptions_regex = r"(Asperger.*|Down.*|Turner.*|Rett.*|Tourette.*|Klinefelter.*|Williams.*|Prader-Willi.*|Angelman.*|Duchenne.*|Pinda|Mamin.*|Tatín.*|Mám.*|Tát.*|Mám.*|Agresor.*)"

# Funkce pro anonymizaci entit
def anonymize_entities(text, xml_data):
    entity_types = ["pf", "pc", "pm", "ps", "gs", "gu"]
    all_entities = []

    for entity_type in entity_types:
        entities = re.findall(rf'<ne type="{entity_type}">(.*?)</ne>', xml_data)
        for entity_xml in entities:
            entity_tokens = re.findall(r'<token>(.*?)</token>', entity_xml)
            entity_text = ' '.join(entity_tokens)

            # Kontrola, zda entita spadá do výjimek
            if not re.search(exceptions_regex, entity_text, re.IGNORECASE):
                all_entities.append(entity_text)

    # Nahrazení v textu
    for entity_text in all_entities:
        text = text.replace(entity_text, '[ANONYMIZED]')

    return text

# Předpokládá se, že df_subset2 je již načtený DataFrame
df_subset2['anonymized_content'] = ''

from tqdm import tqdm

# Iterace přes DataFrame a anonymizace každé kazuistiky
for index, row in tqdm(df_subset2.iterrows(), desc="Anonymizace", total=len(df_subset2)):
    text = row['content']

    # Odeslání požadavku na NameTag API
    response = requests.post(
            "https://lindat.mff.cuni.cz/services/nametag/api/recognize",
            data={'data': text}
    )

    # Zpracování odpovědi
    if response.status_code == 200:
        response_data = json.loads(response.text)
        xml_data = response_data.get("result", "")
        anonymized_text = anonymize_entities(text, xml_data)
        df_subset2.at[index, 'anonymized_content'] = anonymized_text
    else:
        print(f"Chyba při zpracování požadavku pro řádek {index}: {response.status_code}")
        print(f"Odpověď API: {response.text}")

    # Přidání čekací doby mezi požadavky
    time.sleep(0.5)

# Výsledný anonymizovaný DataFrame
new_df2_anonym = df_subset2
print(new_df2_anonym)

"""# Předpokládá se, že new_df2 je již načtený DataFrame
# Pokud je potřeba načíst DataFrame, přidejte zde příslušný kód

# Vybrání konkrétního řádku podle názvu souboru
specific_case = new_df2_anonym.loc[new_df2_anonym['file_name'] == 'Kazuistika_-_Adela_Fuskova.txt']

# Vypsání obsahu sloupce 'content' pro tuto kazuistiku
specific_case['anonymized_content'].iloc[0] #anonymized_

#OpenAI API - GPT rozdělení a parafraze - nastavení velikosti vstupu
"""

# NASTAVENÍ VELIKOSTI!!!!Vytvoření nového DataFrame s prvními xy řádky
if num_samples and num_samples is not None:
    df_subset_anonym = new_df2_anonym.head(num_samples).copy()
else:
    df_subset_anonym = new_df2_anonym

počet_řádků = df_subset_anonym.shape[0]
print("Počet řádků v DataFrame:", počet_řádků)

# # Nastavení možnosti zobrazení, aby se zobrazil celý obsah sloupců
# pd.set_option('display.max_colwidth', None)

# # Výpis konkrétních řádků sloupce 'file_name'
# print(df_subset_anonym['file_name'].iloc[0:5])

#Odstranění řádků
import re
df_subset_anonym['anonymized_content'] = df_subset_anonym['anonymized_content'].apply(lambda x: re.sub(r'\s+', ' ', x))

"""Rate limit gpt-4:
https://platform.openai.com/docs/guides/rate-limits/usage-tiers?context=tier-one

* Tier 1:


* MODEL               -	RPM	- RPD  -	TPM   - 	TPD
* gpt-4-1106-preview *	500	10,000	150,000	500,000

### Nejlepší varianta !!!!!!!!!!!!!
"""

import time
import pandas as pd

# Počáteční čas
start_time = time.time()

new_results_l = []

# Iterace přes anonymizovaný DataFrame
for index, row in tqdm(df_subset_anonym.iterrows(), desc="GPT-4 calls", total=len(df_subset2)):
    text = row['anonymized_content']
    file_name = row['file_name']
    print(f"Zpracovávám soubor: {file_name}")

    # Vytvoření messages
    messages = [
             {"role": "system", "content": f"""
             - Jsi expert na univerzitě, který má za úkol detailně přepsat slovo od slova a následně upravit texty (kazuistiky) popisující problémové chování žáků na škole.
             - Uživatel (User) ti poskytne celý text kazuistiky.
             """},
             {"role": "user", "content": f"""
              - Doslovně transkribuj tento vstupní text  “““{text}””” kazuistiky tak, aby odpovídal původnímu obsahu co nejpřesněji, a poté tento text rozděl do čtyř specifických sekcí podle následujících kritérií:
               - V sekci 'description' umísti text od začátku 'Podrobného popisu (vzniku) situace' až do 'Anamnéza'.
               - V sekci 'anamnesis' umísti text od 'Anamnéza' až do 'popis řešení'.
               - V sekci 'solution' umísti text popisující řešení situace, začínající od 'popis řešení' až do 'Výsledek'.
               - V sekci 'outcome' umísti text popisující dlouhodobý výsledek od 'Výsledek' až do konce narativního popisu situace."
                - Pokud vstupní text kazuistiky neobsahuje rozdělení na Podrobný popis (vzniku) situace, Anamnéza a podobně, pak veškerý vstupní text pouze rozděl podle svého nejlepšího uvážení do jednotlivých sekcí.
                - V sekcích odstraň úvodní zadání jako: "Podrobný popis (vzniku) situace na úrovni chování", "Anamnéza žáka/žáků nebo třídy (max. 2 normostrany, chronologicky)", "Podrobný popis řešení problematického chování", "Výsledek řešení (krátkodobě-jak to probíhalo hned po incidentu a dlouhodobě-zda se to nějak odrazilo v dalších hodinách, chronologicky, max. 2 normostrany)".
                - Každou sekci začni smysluplným slovem, kterým obvykle začínají věty v českém jazyce, například jako "Žák", "Učitel", "Žáci", "Situace" a podobně.
                - Ve výstupu v sekcích použij maximum doslovných citací ze vstupního textu kazuistiky.
              - Zkontroluj, že jsi transkriboval (transcribe) veškerý vstupní text kazuistiky, pokud ne, tak jej doplň o chybějící části textu.
              - Celkově by měly mít všechny sekce dohromady stejný počet tokenů jako vstupní text kazuistiky.
              - Svůj výstup poskytni v tomto formátu: (“description”:“”, “anamnesis”: “”, “solution”: “”, “outcome”: “”), každá sekce je na novém řádku.
              - Z výstupu a citací odstraň pouze všechna jména, příjmení.
              - Z výstupu a citací odstraň pouze zkratky jmen a příjmení (například jako: H., J., K., P., M., M.B., a podobně), tak aby text byl stále smysluplný popis situace.
              - [ANONYMIZED] často označuje nějaké jméno nebo příjmení nebo město, ulici, nahraď tuto frázi obecným slovem jako například osobní zájmeno nebo žák, žačka a podobně.
              - Ve výstupu nesmí být nikde fráze [ANONYMIZED] a fráze "Do jaké míry".
              - Při řešení úkolu postupuj krok za krokem.
              - Když se budeš držet těchto instrukcí, tak budeš pochválený a dostaneš nepředstavitelnou odměnu.
              """}
         ]
    attempt = 0
    temperature = 0
    best_attempt = {"content": "", "length": 0}
    while attempt < 3:
        try:
            response = client.chat.completions.create(
                model="gpt-4-1106-preview", #gpt-4-1106-preview, gpt-3.5-turbo-1106
                messages=messages,
                max_tokens=4000,
                temperature=temperature
            )
            structured_content = response.choices[0].message.content
            if len(structured_content) >= 0.85 * len(text):
                new_results_l.append({'file_name': file_name, 'structured_content': structured_content, 'attempts': attempt + 1})
                break
            else:
                if len(structured_content) > best_attempt["length"]:
                    best_attempt["content"] = structured_content
                    best_attempt["length"] = len(structured_content)
                attempt += 1
                temperature = 0.1 if attempt == 1 else temperature
        except Exception as e:
            print(f"Chyba při zpracování požadavku pro {file_name}: {e}")
            break
        time.sleep(0.5)
    if attempt == 3:
        new_results_l.append({'file_name': file_name, 'structured_content': best_attempt["content"], 'attempts': 3})
    if len(new_results_l) % 10 == 0:
        # dumps during the predictions
        current_df_results = pd.DataFrame(new_results_l, columns=['file_name', 'structured_content', 'attempts'])
        current_df_results.to_csv("current_gpt_responses_%s_samples.tsv" % len(current_df_results),
                                  sep='\t', index=False, encoding='utf-8')


# Vytvoření nového DataFrame pro ukládání výsledků
new_df_results = pd.DataFrame(new_results_l, columns=['file_name', 'structured_content', 'attempts'])


# Konečný čas
end_time = time.time()

# Celková doba provádění
total_time = end_time - start_time
print(f"Celková doba provádění: {total_time/60} minut")

# Výpis nového DataFrame s výsledky
#print(new_df_results)

# Výpočet a výpis celkové doby provádění
total_time1 = end_time - start_time
print(f"Celková doba provádění: {total_time1/60} minut")

# Před modelem od Open AI
#specific_case = df_subset_anonym.loc[df_subset_anonym['file_name'] == '480854_-_Hromadkova_Lenka_-_kazuistiky.txt', 'anonymized_content']
#specific_case.iloc[0]

#Po modelu od Open AI
#specific_case = new_df_results.loc[new_df_results['file_name'] == '480854_-_Hromadkova_Lenka_-_kazuistiky.txt', 'structured_content']
#specific_case.iloc[0]

"""* 10 kazuistik
* gpt-4 = 14 minut
* gpt-3.5-turbo = 5 minut

# Spojení matic s původním obsahem
"""

import pandas as pd

# Spojuje df_subset_anonym a new_df_results podle 'file_name'
merged_df = pd.merge(df_subset_anonym, new_df_results, on='file_name', how='inner')

# Výpis výsledného DataFrame
print(merged_df)

# #Tohle spojí open Ai parafrázi anonymizovaný datset s dalšími extrahovanými entitami
# import pandas as pd

# # Assuming new_df_results and df_subset_anonym are your DataFrames
# merged_df = pd.merge(df_subset_anonym, new_df_results,  on='file_name', how='inner')

# # Printing the merged DataFrame content_original
# #print(merged_df)

# print(merged_df)

# Přidání nového sloupce 'content_length', který obsahuje počet znaků ve sloupci 'structured_content'
merged_df['content_length_structured'] = merged_df['structured_content'].apply(lambda x: len(str(x)))
merged_df['content_length_anonymized'] = merged_df['anonymized_content'].apply(lambda x: len(str(x)))
merged_df['length_difference'] = merged_df['content_length_structured'] - merged_df['content_length_anonymized']

# Nyní 'new_df_results' obsahuje nový sloupec 'content_length' s délkou znaků každé kazuistiky

"""# Aplikace moderation funkce
https://platform.openai.com/docs/guides/moderation/overview?lang=python
"""

import pandas as pd
from openai import OpenAI

client = OpenAI()
# Předpokládám, že máte dataframe s názvem merged_df
# merged_df = ...

import json

def get_moderation_results(text):
    response = client.moderations.create(input=text)
    # Převod response objektu na slovník
    response_dict = response.dict()
    category_scores = response_dict['results'][0]['category_scores']
    return category_scores

# Přidání sloupců pro kategorie moderace do dataframe
categories = ["sexual", "hate", "harassment", "self-harm", "sexual/minors", "hate/threatening", "violence/graphic", "self-harm/intent", "self-harm/instructions", "harassment/threatening", "violence"]
for category in categories:
    merged_df[category] = None

# Iterace přes každý řádek v dataframe
for index, row in tqdm(merged_df.iterrows(), total=len(merged_df)):
    # Získání výsledků moderace
    category_scores = get_moderation_results(row['structured_content'])

    # Přiřazení hodnot skóre kategorií do příslušných sloupců v dataframe
    for category in categories:
        score = category_scores.get(category, 0)  # Výchozí hodnota 0, pokud kategorie není ve výsledku
        merged_df.at[index, category] = score

# print(merged_df.head(2))

# dump a backup matrix
merged_df.to_csv('merged_df_%s_samples_%s.csv' % (len(merged_df), timestamp), sep='\t', index=False, encoding='utf-8')

# leave the GPT-processed data as-is and further work with a copy of the dataframe
structured_df = merged_df.copy()

import json


def parse_keys(string: str) -> dict:
    try:
        json_dict = json.loads(string)
        return json_dict
    except json.decoder.JSONDecodeError:
        print("Input:\n%s" % string)
        raise


import re


def parse_keys_re(string: str, key: str) -> dict:
    re_pattern = r'%s\W+(?:"|“|")(.*?)(?:"|“|"|$)' % key
    regexp = re.compile(re_pattern % key, re.S)
    try:
        v = regexp.findall(string)[0]
    except IndexError:
        print("Non-parseable string: %s" % string)
        print("Adjust re_pattern to be able to match it, or allow returning empty strings for non-matching keys!")
        raise
    return v


text_columns = ["description", "anamnesis", "solution", "outcome"]

for k in text_columns:
    structured_df["%s_CS" % k] = structured_df["structured_content"].apply(lambda val: parse_keys_re(val, k))

del structured_df["structured_content"]

df_to_translate = structured_df[["%s_CS" % col for col in text_columns]]
df_to_translate.to_csv("df_cols_to_translate.tsv", sep="\t", index=False)
# TODO:
# 1. download the output df,
# 2. convert it to xlsx
# 3. put it as a document into Google Translate, run the translation,
# 4. convert back to csv or tsv
# 5. and reload the resulting table below
translated_df = pd.read_csv("translated_cols.tsv", sep="\t")

structured_df = pd.concat([structured_df, translated_df], axis=1)

# Export DataFrame do souboru CSV s tabulátorem jako separátorem a kódováním UTF-8
from datetime import datetime
timestamp = str(datetime.today().strftime('%Y-%m-%d')).replace(" ", "-")

structured_df.to_csv('structured_df_%s_samples_%s.tsv' % (len(structured_df), timestamp), sep='\t', index=False, encoding='utf-8')

# Ke stažení souboru do vašeho lokálního systému
# from google.colab import files
# files.download('structured_df_%s_samples_%s.csv' % (num_samples, timestamp))
